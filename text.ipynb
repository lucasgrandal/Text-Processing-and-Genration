{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90d1bda-3c36-454b-9495-4da13a17857f",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f65d93-f0c9-4df6-be76-00fdd4ccbd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import os\n",
    "import re\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c812b0-102e-487b-bd86-6e72db1bd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 265\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827647f-04b3-4159-9fc6-f4ba52154bab",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92ea829-0243-4d51-8204-388776f9e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/candide.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/dracula.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/enchiridion.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/grimm_fairy_tales.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/Heimskringla.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/manual_of_gardening.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/meditations.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/miserables.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/poirot_investigates.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/the_adventures_of_sherlock_holmes.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/the_monomaniac.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/under_fire.txt',\n",
    "    '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_train/war_and_peace.txt'\n",
    "]\n",
    "val_path = ['/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_val/a_study_in_scarlet.txt']\n",
    "test_path = [ '/Users/angelgarealamas/Desktop/project3/inf265_v24_project03_data/data_test/Twenty_Thousand_Leagues_under_the_Sea.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637d043-5a0e-4c29-ac6a-3c6cdadf8ed3",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4cc935-2ad3-4375-a8ee-7970a0e46d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize constants and tokenizer\n",
    "TOKENIZER_EN = get_tokenizer('basic_english')\n",
    "PATH_GENERATED = './generated/'\n",
    "MIN_FREQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf103e3-bfa5-4e65-afc5-91194dcbbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(file_paths):\n",
    "    \"\"\"\n",
    "    Read files from a list of file paths and return their contents as a list of lines.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines.extend(file.readlines())\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {str(e)}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def tokenize(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "\n",
    "def yield_tokens(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    # Match any word containing digit\n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    # Match word containing a uppercase \n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    # Match any sequence containing more than one space\n",
    "    no_spaces = '\\s+'\n",
    "    \n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits, ' ', line)\n",
    "        line = re.sub(no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \n",
    "    Useful to get some insight on the data and to compute loss weights\n",
    "    \"\"\"\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    # vocab contains the vocabulary found in the data, associating an index to each word\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    # Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "    vocab.append_token(\"i\")\n",
    "    # Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe624bcb-0c98-4a4e-867f-d25f0f6625ab",
   "metadata": {},
   "source": [
    "### Read and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea277eb-2ad3-4c3a-ae6a-135f7c445a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines train: 268380\n",
      "Total number of tokens train: 2684706\n",
      "Total number of lines val: 4649\n",
      "Total number of tokens val: 49526\n",
      "Total number of lines test: 12696\n",
      "Total number of tokens test: 124152\n"
     ]
    }
   ],
   "source": [
    "# Read and tokenize training data\n",
    "train_lines = read_files(train_paths)\n",
    "train_tokens = tokenize(train_lines)\n",
    "\n",
    "# Read and tokenize validation data\n",
    "val_lines = read_files(val_path)\n",
    "val_tokens = tokenize(val_lines)\n",
    "\n",
    "# Read and tokenize test data\n",
    "test_lines = read_files(test_path)\n",
    "test_tokens = tokenize(test_lines)\n",
    "\n",
    "# Show some basic statistics\n",
    "print(f\"Total number of lines train: {len(train_lines)}\")\n",
    "print(f\"Total number of tokens train: {len(train_tokens)}\")\n",
    "\n",
    "print(f\"Total number of lines val: {len(val_lines)}\")\n",
    "print(f\"Total number of tokens val: {len(val_tokens)}\")\n",
    "\n",
    "print(f\"Total number of lines test: {len(test_lines)}\")\n",
    "print(f\"Total number of tokens test: {len(test_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cba21b-1dd7-423c-b304-84f76f9cbda1",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ecf6918-d7ac-4f40-ad0a-04bc0c65b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the training dataset: 2684706\n",
      "Number of distinct tokens in the training dataset: 52105\n",
      "Size of the vocabulary (words appearing at least 100 times): 1880\n"
     ]
    }
   ],
   "source": [
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "# Load vocabulary if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    train_vocab = torch.load(PATH_GENERATED + VOCAB_FNAME)\n",
    "else:\n",
    "    # Create vocabulary based on the words in the training dataset\n",
    "    train_vocab = create_vocabulary(train_lines, MIN_FREQ)\n",
    "    torch.save(train_vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "# Count the frequency of each word in the vocabulary\n",
    "train_freqs = count_freqs(train_tokens, train_vocab)\n",
    "\n",
    "# Calculate and display the desired statistics\n",
    "total_tokens_train = len(train_tokens)  # Total number of tokens in the training dataset\n",
    "distinct_tokens_train = len(set(train_tokens))  # Number of distinct tokens in the training dataset\n",
    "vocab_size = len(train_vocab)  # Size of the vocabulary\n",
    "\n",
    "# Displaying the calculated statistics\n",
    "print(f\"Total number of tokens in the training dataset: {total_tokens_train}\")\n",
    "print(f\"Number of distinct tokens in the training dataset: {distinct_tokens_train}\")\n",
    "print(f\"Size of the vocabulary (words appearing at least {MIN_FREQ} times): {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa04e2-6209-4fb6-82f1-8293e7730d2c",
   "metadata": {},
   "source": [
    "### Analyze vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb352127-97de-4d8d-bf7d-b0e2f9e56207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Frequencies:\n",
      ",: 182537\n",
      "the: 151278\n",
      ".: 123727\n",
      "and: 82289\n",
      "of: 65661\n",
      "to: 62763\n",
      "a: 49230\n",
      "in: 41477\n",
      "he: 37167\n",
      "that: 31052\n",
      "\n",
      "Vocabulary covers 83.84% of the total words in the dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rare tokens (appearing only once): 21986\n",
      "Example rare tokens: ['nosepiece', 'narshkina', 'joinings', 'billet-doux', 'canny', 'saint-saulve', 'luffa', '$24', 'lartif', 'post-haste']\n",
      "\n",
      "Number of common tokens (appearing at least 100 times): 2091\n",
      "Example common tokens: ['shoulder', 'quarters', 'walked', 'front', 'court', 'staff', 'chosen', 'public', 'repeated', 'friendship']\n"
     ]
    }
   ],
   "source": [
    "def analyze_vocabulary(tokens, vocab, min_freq):\n",
    "    # Count occurrences of each word in the token list\n",
    "    token_counts = {}\n",
    "    for token in tokens:\n",
    "        if token in token_counts:\n",
    "            token_counts[token] += 1\n",
    "        else:\n",
    "            token_counts[token] = 1\n",
    "\n",
    "    # Calculate the frequency of words in the vocabulary\n",
    "    vocab_freqs = {word: token_counts.get(word, 0) for word, idx in vocab.get_stoi().items()}\n",
    "\n",
    "    # Display word frequencies\n",
    "    print(\"\\nWord Frequencies:\")\n",
    "    for word, freq in sorted(vocab_freqs.items(), key=lambda item: item[1], reverse=True)[:10]:  # top 10 words\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "    # Calculate and display vocabulary coverage\n",
    "    total_words = sum(token_counts.values())\n",
    "    covered_words = sum(vocab_freqs.values())\n",
    "    coverage = (covered_words / total_words) * 100\n",
    "    print(f\"\\nVocabulary covers {coverage:.2f}% of the total words in the dataset\")\n",
    "\n",
    "    # Analyze token lengths\n",
    "    token_lengths = [len(token) for token in tokens]\n",
    "    plt.figure()\n",
    "    plt.hist(token_lengths, bins=range(1, max(token_lengths) + 1), edgecolor='black')\n",
    "    plt.title('Token Length Distribution')\n",
    "    plt.xlabel('Length of Token')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Frequency distribution of tokens\n",
    "    freq_values = list(token_counts.values())\n",
    "    plt.figure()\n",
    "    plt.hist(freq_values, bins=range(1, max(freq_values) + 1, max(freq_values) // 50), log=True, edgecolor='black')\n",
    "    plt.title('Token Frequency Distribution')\n",
    "    plt.xlabel('Frequency of Token')\n",
    "    plt.ylabel('Number of Tokens')\n",
    "    plt.show()\n",
    "\n",
    "    # Information about rare and common tokens\n",
    "    rare_tokens = {word for word, freq in token_counts.items() if freq == 1}\n",
    "    common_tokens = {word for word, freq in token_counts.items() if freq >= 100}\n",
    "\n",
    "    print(f\"\\nNumber of rare tokens (appearing only once): {len(rare_tokens)}\")\n",
    "    print(f\"Example rare tokens: {list(rare_tokens)[:10]}\")  # showing 10 rare tokens\n",
    "    print(f\"\\nNumber of common tokens (appearing at least 100 times): {len(common_tokens)}\")\n",
    "    print(f\"Example common tokens: {list(common_tokens)[:10]}\")  # showing 10 common tokens\n",
    "\n",
    "    return vocab_freqs, token_counts\n",
    "\n",
    "# Apply the function to the training data\n",
    "train_vocab_freqs, train_token_counts = analyze_vocabulary(train_tokens, train_vocab, MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a71638-8584-4c96-95e1-a8b461ee6981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence with a rare word:\n",
      "it is the independence of personalityfree will.\n",
      "\n",
      "Example sentence with a common word:\n",
      "conscious.\n",
      "\n",
      "Most Common Bigrams:\n",
      "(',', 'and'): 35260\n",
      "('of', 'the'): 17655\n",
      "('in', 'the'): 12133\n",
      "('.', 'the'): 11791\n",
      "('to', 'the'): 8279\n",
      "(',', 'the'): 7643\n",
      "('.', 'he'): 7192\n",
      "('on', 'the'): 5849\n",
      "('and', 'the'): 5746\n",
      "('.', 'i'): 5709\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entropy of the dataset: 9.42 bits per word\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_complexity(tokens, vocab):\n",
    "    # Calculate and plot cumulative coverage by vocabulary\n",
    "    token_frequency = Counter(tokens)\n",
    "    ordered_tokens = sorted(token_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    cumulative_tokens = np.cumsum([freq for _, freq in ordered_tokens])\n",
    "    total_tokens = cumulative_tokens[-1]\n",
    "    cumulative_coverage = cumulative_tokens / total_tokens\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(cumulative_coverage)\n",
    "    plt.title('Cumulative Coverage by Vocabulary Size')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.ylabel('Cumulative Coverage of Tokens')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Sample usage of rare and common words\n",
    "    rare_words = {word for word, freq in token_frequency.items() if freq == 1}\n",
    "    common_words = {word for word, freq in token_frequency.items() if freq > 100}\n",
    "    sample_sentences = {}\n",
    "    for line in train_lines:\n",
    "        line_tokens = TOKENIZER_EN(line)\n",
    "        if rare_words.intersection(line_tokens):\n",
    "            sample_sentences['rare'] = line.strip()\n",
    "        if common_words.intersection(line_tokens):\n",
    "            sample_sentences['common'] = line.strip()\n",
    "\n",
    "    print(\"\\nExample sentence with a rare word:\")\n",
    "    print(sample_sentences.get('rare', 'No example available'))\n",
    "\n",
    "    print(\"\\nExample sentence with a common word:\")\n",
    "    print(sample_sentences.get('common', 'No example available'))\n",
    "\n",
    "    # Most common bigrams\n",
    "    bigrams = ngrams(tokens, 2)\n",
    "    most_common_bigrams = Counter(bigrams).most_common(10)\n",
    "    print(\"\\nMost Common Bigrams:\")\n",
    "    for bigram, freq in most_common_bigrams:\n",
    "        print(f\"{bigram}: {freq}\")\n",
    "\n",
    "    # Zipf's Law visualization\n",
    "    ranks = range(1, len(ordered_tokens) + 1)\n",
    "    frequencies = [freq for _, freq in ordered_tokens]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.loglog(ranks, frequencies, marker=\"o\")\n",
    "    plt.title('Word Frequencies (Zipf\\'s Law)')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate entropy\n",
    "    probabilities = [freq / total_tokens for freq in frequencies]\n",
    "    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    print(f\"\\nEntropy of the dataset: {entropy:.2f} bits per word\")\n",
    "\n",
    "# Execute the complex analysis\n",
    "analyze_text_complexity(train_tokens, train_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8c75b77-1afe-4859-8f8b-89ce1a9b2f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/angelgarealamas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/angelgarealamas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/angelgarealamas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was: 29046 times\n",
      "had: 20159 times\n",
      "being: 1678 times\n",
      "have: 10205 times\n",
      "been: 5446 times\n",
      "having: 1272 times\n",
      "were: 8219 times\n",
      "is: 19965 times\n",
      "are: 8800 times\n",
      "has: 3817 times\n",
      "am: 2169 times\n",
      "\n",
      "Common contexts for 'am':\n",
      "Displaying 5 of 2169 matches:\n",
      "is for the best in this world , for i am infinitely more touched by your extre\n",
      " girls , and nature s masterpiece ? i am so weak that i cannot stand , said pa\n",
      "my part i shall give it to nobody , i am dying . oh , pangloss ! cried candide\n",
      "blood and fury ! answered the other i am a sailor and born at batavia . four t\n",
      "loss , get me a little wine and oil i am dying . this concussion of the earth \n",
      "\n",
      "Common contexts for 'is':\n",
      "Displaying 5 of 19965 matches:\n",
      "logy . he proved admirably that there is no effect without a cause , and that \n",
      " best of all possible baronesses . it is demonstrable , said he , that things \n",
      "or all being created for an end , all is necessarily for the best end . observ\n",
      "consequently they who assert that all is well have said a foolish thing , they\n",
      "ish thing , they should have said all is for the best . candide listened atten\n",
      "\n",
      "Common contexts for 'are':\n",
      "Displaying 5 of 8800 matches:\n",
      "ings cannot be otherwise than as they are for all being created for an end , al\n",
      "tacles thus we have spectacles . legs are visibly designed for stockings and we\n",
      " and of your merit never pay anything are you not five feet five inches high ? \n",
      "r such a man as you to want money men are only born to assist one another . you\n",
      "only born to assist one another . you are right , said candide this is what i w\n",
      "\n",
      "Common contexts for 'was':\n",
      "Displaying 5 of 29046 matches:\n",
      "candide i how candide was brought up in a magnificent castle , \n",
      " in a magnificent castle , and how he was expelled thence . in a castle of west\n",
      "most gentle manners . his countenance was a true picture of his soul . he combi\n",
      "ent with simplicity of spirit , which was the reason , i apprehend , of his bei\n",
      "ough the injuries of time . the baron was one of the most powerful lords in wes\n",
      "\n",
      "Common contexts for 'were':\n",
      "Displaying 5 of 8219 matches:\n",
      " a pack of hounds at need his grooms were his huntsmen and the curate of the v\n",
      "kings and we have stockings . stones were made to be hewn , and to construct c\n",
      "e ought to be the best lodged . pigs were made to be eaten therefore we eat po\n",
      "eck quite down to his rump . as they were going to proceed to a third whipping\n",
      "ry . at length , while the two kings were causing te deum to be sung each in h\n",
      "\n",
      "Common contexts for 'being':\n",
      "Displaying 5 of 1678 matches:\n",
      "as the reason , i apprehend , of his being called candide . the old servants of\n",
      "e otherwise than as they are for all being created for an end , all is necessar\n",
      "oncluded that after the happiness of being born of baron of thunder-ten-tronckh\n",
      "kkeeper . at the end of two months , being obliged to go to lisbon about some m\n",
      " , no one commanded . the anabaptist being upon deck bore a hand when a brutish\n",
      "\n",
      "Common contexts for 'been':\n",
      "Displaying 5 of 5446 matches:\n",
      " of the family suspected him to have been the son of the baron s sister , by a\n",
      "ady would never marry because he had been able to prove only seventy-one quart\n",
      "rest of his genealogical tree having been lost through the injuries of time . \n",
      "st end . observe , that the nose has been formed to bear spectacles thus we ha\n",
      " next he addressed was a man who had been haranguing a large assembly for a wh\n",
      "\n",
      "Common contexts for 'have':\n",
      "Displaying 5 of 10205 matches:\n",
      "vants of the family suspected him to have been the son of the baron s sister ,\n",
      "en formed to bear spectacles thus we have spectacles . legs are visibly design\n",
      "isibly designed for stockings and we have stockings . stones were made to be h\n",
      "tly they who assert that all is well have said a foolish thing , they should h\n",
      "e said a foolish thing , they should have said all is for the best . candide l\n",
      "\n",
      "Common contexts for 'has':\n",
      "Displaying 5 of 3817 matches:\n",
      "he best end . observe , that the nose has been formed to bear spectacles thus w\n",
      "o construct castles therefore my lord has a magnificent castle for the greatest\n",
      "lf before him , cried master pangloss has well said that all is for the best in\n",
      "his terrible plight ! what misfortune has happened to you ? why are you no long\n",
      "he most magnificent of castles ? what has become of miss cunegonde , the pearl \n",
      "\n",
      "Common contexts for 'had':\n",
      "Displaying 5 of 20159 matches:\n",
      "tronckh , lived a youth , whom nature had endowed with the most gentle manners \n",
      "ung lady would never marry because he had been able to prove only seventy-one q\n",
      " lords in westphalia , for his castle had not only a gate , but windows . his g\n",
      "extremely beautiful , though he never had the courage to tell her so . he concl\n",
      "y and very docile . as miss cunegonde had a great disposition for the sciences \n",
      "\n",
      "Common contexts for 'having':\n",
      "Displaying 5 of 1272 matches:\n",
      ", the rest of his genealogical tree having been lost through the injuries of t\n",
      "alled waldberghofftrarbk-dikdorff , having no money , dying of hunger and fati\n",
      "lled and breathing their last after having satisfied the natural wants of bulg\n",
      " him when he arrived in holland but having heard that everybody was rich in th\n",
      "n by the bulgarian soldiers , after having been violated by many they broke th\n"
     ]
    }
   ],
   "source": [
    "# Ensure that NLTK resources are downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def analyze_verbs(tokens, verb_list):\n",
    "    # Filter tokens to only include words (lowercase for uniformity)\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Get parts of speech for each word\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    # Dictionary to hold counts of each verb form\n",
    "    verb_forms = defaultdict(int)\n",
    "\n",
    "    # Count occurrences of each form of the verbs 'be' and 'have'\n",
    "    for word, tag in tagged_words:\n",
    "        if word in verb_list:\n",
    "            verb_forms[word] += 1\n",
    "\n",
    "    # Display the counts of each verb form\n",
    "    for verb, count in verb_forms.items():\n",
    "        print(f\"{verb}: {count} times\")\n",
    "\n",
    "    # Find and display common contexts of these verbs\n",
    "    text = nltk.Text(tokens)\n",
    "    for verb in verb_list:\n",
    "        print(f\"\\nCommon contexts for '{verb}':\")\n",
    "        text.concordance(verb, lines=5)  # Display 5 examples of each\n",
    "\n",
    "# List of common forms of 'be' and 'have'\n",
    "be_forms = ['am', 'is', 'are', 'was', 'were', 'being', 'been']\n",
    "have_forms = ['have', 'has', 'had', 'having']\n",
    "\n",
    "# Collect all relevant forms in one list for analysis\n",
    "all_verbs = be_forms + have_forms\n",
    "\n",
    "# Execute the verb analysis\n",
    "analyze_verbs(train_tokens, all_verbs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e12e14-97c2-49cf-9c25-c4db3f3f0c00",
   "metadata": {},
   "source": [
    "### Train and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "559be90f-f3c8-4067-9f1a-9d48f4aa640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device(device=None):\n",
    "    \"\"\"\n",
    "    Helper function to set device\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"On device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "    device = set_device(device)\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, targets in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "def compute_accuracy(model, loader, device=None):\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts = contexts.to(device)  # Move data to the appropriate device\n",
    "            targets = targets.to(device)    # Move targets to the appropriate device\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "            total += targets.size(0)                 # Increment the total count\n",
    "            correct += (predicted == targets).sum().item()  # Increment the correct count\n",
    "\n",
    "    accuracy = correct / total  # Calculate accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58393b4-0006-42fe-ae07-b38223854616",
   "metadata": {},
   "source": [
    "### Compute labels and create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af69dabe-0134-46ad-9a39-60361ed20530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label(w):\n",
    "    if w == '<unk>':\n",
    "        return 0\n",
    "    elif w in [',', '.', '(', ')', '?', '!']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# It must execute after having created the vocabulary\n",
    "MAP_TARGET = {\n",
    "    train_vocab[w]: compute_label(w) for w in train_vocab.get_itos()\n",
    "}\n",
    "\n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "def create_dataset(text, vocab, context_size=3, map_target=MAP_TARGET):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    n_text = len(text)\n",
    "    \n",
    "    # Transform the text into a list of integers, ensuring that only words in the vocabulary are included\n",
    "    txt = [vocab[token] if token in vocab.get_stoi() else vocab['<unk>'] for token in text]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        # Context son las 'context_size' palabras antes de la palabra objetivo\n",
    "        context = txt[i:i + context_size]\n",
    "        # Palabra usada como target\n",
    "        target = txt[i + context_size]\n",
    "\n",
    "        # Asegurarse de que el target est en map_target\n",
    "        if target in map_target:\n",
    "            targets.append(map_target[target])\n",
    "            contexts.append(torch.tensor(context, dtype=torch.long))\n",
    "\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    # Create a PyTorch dataset with these context/target pairs\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(words, vocab, fname):\n",
    "    # Asegurarse de que el directorio existe\n",
    "    if not os.path.exists(PATH_GENERATED):\n",
    "        os.makedirs(PATH_GENERATED)\n",
    "    \n",
    "    # Si ya est generado\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Crear dataset de contexto/target basado en la lista de strings\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ba22a-1a8b-4f12-8cf3-8a0b8a315d56",
   "metadata": {},
   "source": [
    "### Task 2.1: Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ac3d9b9-a3cf-4359-8ddf-5e9977d05c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_dataset(train_tokens, train_vocab, \"data_train.pt\")\n",
    "data_val = load_dataset(val_tokens, train_vocab, \"data_val.pt\")\n",
    "data_test = load_dataset(test_tokens, train_vocab, \"data_test.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d3c267e-0adc-474c-85f7-a42e82ae837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.load(\"/Users/angelgarealamas/Desktop/3old/generated/data_train.pt\")\n",
    "data_val = torch.load(\"/Users/angelgarealamas/Desktop/3old/generated/data_val.pt\")\n",
    "data_test = torch.load(\"/Users/angelgarealamas/Desktop/3old/generated/data_test.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "193ec3e6-23f3-43cc-8c4c-a34540360784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model\n",
    "embedding_layer = nn.Embedding(len(train_vocab), 50)  \n",
    "# Randomly initialize embedding weights or load your pretrained weights here\n",
    "embedding_layer.weight.data.copy_(torch.randn(embedding_layer.weight.shape))\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the number of epochs\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ecf87618-b467-475e-9cf9-cf22a31ade1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, nhead=5, num_encoder_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        embedded = self.embedding(x)  # Embedding layer output\n",
    "        if mask is not None:\n",
    "            embedded = embedded.transpose(0, 1)  # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
    "            output = self.transformer_encoder(embedded, src_key_padding_mask=mask)\n",
    "            output = output.transpose(0, 1)  # Back to (batch_size, sequence_length, embedding_dim)\n",
    "        else:\n",
    "            embedded = embedded.transpose(0, 1)  # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
    "            output = self.transformer_encoder(embedded)\n",
    "            output = output.transpose(0, 1)  # Back to (batch_size, sequence_length, embedding_dim)\n",
    "        return output[:, -1, :]  # Return embeddings of the last token in the sequence\n",
    "\n",
    "# Example usage of the TransformerEmbedder\n",
    "vocab_size = len(train_vocab)  # Use your actual vocabulary size\n",
    "embedding_dim = 15  # Example embedding size, adjust as needed\n",
    "model1 = TransformerEmbedder(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6bf0ec12-f1c4-4ae2-9095-5c5362c76473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, model1\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m losses_train \u001b[38;5;241m=\u001b[39m train(n_epochs, optimizer, model1, loss_fn, train_loader, device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# After training you can evaluate the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m contexts \u001b[38;5;241m=\u001b[39m contexts\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     25\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(contexts)\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[78], line 17\u001b[0m, in \u001b[0;36mTransformerEmbedder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m embedded\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Transformer expects (sequence_length, batch_size, embedding_dim)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(embedded)\n\u001b[1;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Back to (batch_size, sequence_length, embedding_dim)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, src_mask\u001b[38;5;241m=\u001b[39mmask, is_causal\u001b[38;5;241m=\u001b[39mis_causal, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:730\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 730\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))))\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1.to(device) # Send the model to the device\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model1.parameters()), lr=0.001)\n",
    "\n",
    "# Start training\n",
    "losses_train = train(n_epochs, optimizer, model1, loss_fn, train_loader, device)\n",
    "\n",
    "# Evaluate the model over the validation set\n",
    "print(\"Testing model\")\n",
    "model1_accuracy = compute_accuracy(model1, val_loader, device)\n",
    "print(f\"Test accuracy: {model1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bde3eaf8-2042-460e-b90d-5e4500a7227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, model2\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m losses_train \u001b[38;5;241m=\u001b[39m train(n_epochs, optimizer, model2, loss_fn, train_loader, device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# After training you can evaluate the model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(contexts)\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[0;32m---> 30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# COnfigure hyperparameters\n",
    "vocab_size = len(train_vocab)  # Use your actual vocabulary size\n",
    "embedding_dim = 10  # Example embedding size, adjust as needed\n",
    "num_encoder_layers=2\n",
    "model2 = TransformerEmbedder(vocab_size, embedding_dim)\n",
    "\n",
    "model2.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model2.parameters()), lr=0.1)\n",
    "\n",
    "# Start training\n",
    "losses_train = train(n_epochs, optimizer, model2, loss_fn, train_loader, device)\n",
    "\n",
    "# Evaluate the model over the validation set\n",
    "print(\"Testing model\")\n",
    "model2_accuracy = compute_accuracy(model2, val_loader, device)\n",
    "print(f\"Test accuracy: {model2_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "246ce71d-775b-47cf-b5b4-376d89bba1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbeddingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=100):\n",
    "        super(SimpleEmbeddingNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)  # Output layer size is vocab_size for prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Embed the input words\n",
    "        x = torch.mean(x, dim=1)  # Average the embeddings (simple pooling)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed1126d6-fdcd-4105-83a2-20ba84b1b07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model3\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Iniciar el entrenamiento\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m losses_train \u001b[38;5;241m=\u001b[39m train(n_epochs, optimizer, model3, loss_fn, train_loader, device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo despus del entrenamiento\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluando modelo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(contexts)\n\u001b[0;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set model hyperparameters\n",
    "vocab_size = len(train_vocab)  # Assume train_vocab is your vocabulary\n",
    "embedding_dim = 50  # Embedding dimension\n",
    "hidden_dim = 100  # Hidden layer demansion\n",
    "model3 = SimpleEmbeddingNet(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "model3.to(device)  # Move the model to the device\n",
    "\n",
    "# Define loss fn and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=0.001)\n",
    "\n",
    "# Start Tranining\n",
    "losses_train = train(n_epochs, optimizer, model3, loss_fn, train_loader, device)\n",
    "\n",
    "# Evaluate the model over the validation set\n",
    "print(\"Evaluando modelo\")\n",
    "model3_accuracy = compute_accuracy(model3, val_loader, device)\n",
    "print(f\"Precisin del modelo: {model3_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc5897b4-9e7b-4b22-8f97-9da561e4f19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model4\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Iniciar el entrenamiento\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m losses_train \u001b[38;5;241m=\u001b[39m train(n_epochs, optimizer, model4, loss_fn, train_loader, device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo despus del entrenamiento\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluando modelo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure the hyperparameters\n",
    "vocab_size = len(train_vocab)  # Asume que train_vocab es tu vocabulario\n",
    "embedding_dim = 100  # Dimensiones del embedding\n",
    "hidden_dim = 300  # Dimensiones de la capa oculta\n",
    "model4 = SimpleEmbeddingNet(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "model4.to(device)  # Mueve el modelo al dispositivo adecuado\n",
    "\n",
    "# DEfine loss fn and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model4.parameters(), lr=0.001)\n",
    "\n",
    "# Start training\n",
    "losses_train = train(n_epochs, optimizer, model4, loss_fn, train_loader, device)\n",
    "\n",
    "# Evaluate the model over the validation set\n",
    "print(\"Evaluando modelo\")\n",
    "model4_accuracy = compute_accuracy(model4, val_loader, device)\n",
    "print(f\"Precisin del modelo: {model4_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fed9995-6dd5-4ade-aadd-2e8bad0fd2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo es model2 con una precisin de 0.7419784746481433\n"
     ]
    }
   ],
   "source": [
    "# List of all models and their accuracies\n",
    "model_accuracies = [model1_accuracy1, model2_accuracy, model3_accuracy, model4_accuracy]\n",
    "                    \n",
    "# Find the highest accuracy\n",
    "max_accuracy = max(model_accuracies)\n",
    "\n",
    "# Find the index of the model with the best accuracy\n",
    "best_model_index = model_accuracies.index(max_accuracy)\n",
    "\n",
    "# Assign to best model the name of the best model, we suppose all models are numbered: model1...model8\n",
    "best_model = f'model{best_model_index + 1}'\n",
    "\n",
    "print(f'The best model is {best_model} with an accuracy over the validation set of {max_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2831f0db-9037-47ae-9749-afe95d417452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'house': ['world', 'room', 'experience', 'country', 'wants', 'weeks', 'hall', 'within', 'pity', 'falling']\n",
      "Most similar words to 'white': ['regular', 'some', ')', 'hideous', 'killed', 'heavy', 'into', 'two', 'handsome', 'its']\n",
      "Most similar words to 'man': ['woman', 'sit', 'whether', 'child', 'sign', 'details', 'husband', 'struggle', 'soldier', 'who']\n",
      "Most similar words to 'have': ['has', 'hast', 'whether', 'having', 'had', 'presented', 'keep', 've', 'carried', 'coming']\n",
      "Most similar words to 'be': ['is', 'existed', 'are', 'been', 'were', 'apart', 'escape', 'was', '!', 'quite']\n",
      "Most similar words to 'child': ['man', 'woman', 'places', 'soldier', 'husband', 'wants', 'artillery', 'rope', 'officer', 'boy']\n",
      "Most similar words to 'yes': ['well', 'stop', 'listen', 'replies', 'angrily', 'however', 'understand', 'besides', 'pointing', 'born']\n",
      "Most similar words to 'are': ['was', 'were', 'is', 'been', 'becomes', 'gives', 'be', 'has', 's', 'promised']\n",
      "Most similar words to 'is': ['was', 'were', 'are', 'make', 'and', 'be', 'probably', 'fit', 'has', 'many']\n",
      "Most similar words to 'dog': ['note', 'persons', 'kings', 'figure', 'happiness', 'event', 'cook', 'lawn', 'generals', 'bondes']\n",
      "Most similar words to 'was': ['were', 'is', 'are', 'am', 'became', 'can', 'gives', 'isn', 'fit', 'has']\n",
      "Most similar words to 'happy': ['watched', 'abandoned', 'running', 'relations', 'pleasure', 'doing', 'hanging', 'rising', 'sitting', 'eaten']\n",
      "Most similar words to 'table': ['bed', 'lips', 'kingdom', 'tomb', 'cloak', 'killed', 'frame', 'ago', 'soon', 'thing']\n",
      "Most similar words to 'sea': ['lips', 'bed', 'pity', 'ear', 'answer', 'learn', 'ships', 'throw', 'bird', 'forest']\n",
      "Most similar words to 'water': ['building', 'birds', 'sometimes', 'post', 'terror', 'farther', 'sea', 'seeds', 'bread', 'kitchen']\n",
      "Most similar words to 'black': ['heavy', 'has', 'the', 'any', 'chance', 'free', 'this', 'half', 'simple', 'evidently']\n",
      "Most similar words to 'said': ['says', 'answered', 'replied', 'cried', 'continued', 'added', 'murmured', 'resumed', 'remarked', 'usual']\n"
     ]
    }
   ],
   "source": [
    "#we manually select the best model\n",
    "best_model = model2\n",
    "\n",
    "def get_embeddings(vocab, embedding_layer):\n",
    "    # Retrieve the embeddings for the entire vocabulary\n",
    "    embeddings = embedding_layer.weight.data\n",
    "    # Normalize the embeddings\n",
    "    norm_embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "    return norm_embeddings\n",
    "\n",
    "def cosine_similarity(embeddings):\n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix = torch.mm(embeddings, embeddings.t())\n",
    "    return similarity_matrix\n",
    "\n",
    "def most_similar(similarity_matrix, vocab, word, n=10):\n",
    "    # Get the index of the word\n",
    "    idx = vocab[word]\n",
    "    # Get the similarities\n",
    "    similarities = similarity_matrix[idx]\n",
    "    # Exclude the word itself and get the top n similar words\n",
    "    top_indices = similarities.argsort(descending=True)[1:n+1]\n",
    "    similar_words = [vocab.get_itos()[i] for i in top_indices]\n",
    "    return similar_words\n",
    "\n",
    "# Assuming the embedding layer and vocabulary are already defined\n",
    "embeddings = get_embeddings(train_vocab, best_model.embedding)\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Define words for which you want to find the most similar words\n",
    "words_to_check = ['house', 'white', 'man', 'have', 'be', 'child', 'yes', 'are', 'is', 'dog', 'was', 'happy', 'table', 'sea', 'water', \n",
    "                  'black', 'said']\n",
    "\n",
    "# Finding the most similar words for the chosen words\n",
    "similar_words_dict = {}\n",
    "for word in words_to_check:\n",
    "    similar_words_dict[word] = most_similar(similarity_matrix, train_vocab, word, n=10)\n",
    "\n",
    "# Print similar words for each chosen word\n",
    "for word, similar_words in similar_words_dict.items():\n",
    "    print(f\"Most similar words to '{word}': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae0adc81-1b91-4e87-88ba-71c05eaa0d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved: ./embeddings.tsv and ./metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "def save_embeddings_tsv(embeddings, vocab, path='./'):\n",
    "    \"\"\"\n",
    "    Saves embeddings and vocabulary in the format required by TensorFlow Projector.\n",
    "\n",
    "    Args:\n",
    "    - embeddings: Tensor, the embeddings from the model.\n",
    "    - vocab: Vocabulary object from torchtext.\n",
    "    - path: str, directory where to save the files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the path exists\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # File paths\n",
    "    embeddings_file = os.path.join(path, 'embeddings.tsv')\n",
    "    metadata_file = os.path.join(path, 'metadata.tsv')\n",
    "\n",
    "    # Open files\n",
    "    with open(embeddings_file, 'w') as ef, open(metadata_file, 'w') as mf:\n",
    "        for idx, word in enumerate(vocab.get_itos()):\n",
    "            # Embedding vector\n",
    "            vector = embeddings[idx].numpy()\n",
    "            vector_str = '\\t'.join([str(x) for x in vector])\n",
    "            ef.write(f\"{vector_str}\\n\")\n",
    "            # Corresponding word\n",
    "            mf.write(f\"{word}\\n\")\n",
    "\n",
    "    print(f\"Files saved: {embeddings_file} and {metadata_file}\")\n",
    "\n",
    "embeddings = modelll.embedding.weight.data\n",
    "save_embeddings_tsv(embeddings, train_vocab, path='./')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb86d30-94c1-4c9a-acc7-698e9020b0ed",
   "metadata": {},
   "source": [
    "### Part 2.2: Conjugating be and have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "48fac41f-08a4-432f-a623-d65d5d1cf8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model with appropriate output dimension for 12 classes\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * CONTEXT_SIZE, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 12)  # Output dimension is 12 for 12 forms of 'be' and 'have'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, device, n_epochs=10):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for contexts, targets in train_loader:\n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch}, Training Loss: {total_loss / len(train_loader)}')\n",
    "        validate_model(model, val_loader, device)\n",
    "\n",
    "# Function to validate the model\n",
    "def validate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    total = correct = 0\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    print(f'Validation Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e0ecc76b-4a36-40c8-b8c0-c686f23cbda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.6492494540821654\n",
      "Validation Accuracy: 74.10899985865153%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(data_val, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_model(model1, data_loader, val_loader, device, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[116], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, n_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(contexts)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[0;32m---> 28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preparing the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = MLPModel(len(train_vocab), 100, 300)  # Ensure the vocab size is correct\n",
    "model1.to(device)\n",
    "data_loader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_model(model1, data_loader, val_loader, device, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "73a70cfe-0f08-475f-8b28-63e970a2e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=12):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x[:, -1, :])  # Use the last output only\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d220b84b-7f9e-4e0a-88c2-e317d7692d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.6480890605321172\n",
      "Validation Accuracy: 74.0464026815823%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(data_val, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_model(model2, data_loader, val_loader, device, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[116], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, n_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:392\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    391\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    395\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preparing the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(train_vocab)  # Ensure the vocab size is correct\n",
    "embedding_dim = 100  # Dimensionality of embedding space\n",
    "hidden_dim = 100  # Dimensionality of RNN hidden states\n",
    "\n",
    "# Create the RNN model instance\n",
    "model2 = RNNModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "model2.to(device)\n",
    "data_loader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_model(model2, data_loader, val_loader, device, n_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5fbc3887-8895-4857-bcd0-491f7f825ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=12):\n",
    "        super(MLPAttentionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(embedding_dim * CONTEXT_SIZE, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "99db264c-582d-4407-a056-6e16c60d24f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.657625524457747\n",
      "Validation Accuracy: 74.0766916382287%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(data_val, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_model(model3, data_loader, val_loader, device, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[116], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, n_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m    160\u001b[0m         grads,\n\u001b[1;32m    161\u001b[0m         exp_avgs,\n\u001b[1;32m    162\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:80\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m state_values:\n\u001b[1;32m     78\u001b[0m             s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_group\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     group,\n\u001b[1;32m     83\u001b[0m     params_with_grad,\n\u001b[1;32m     84\u001b[0m     grads,\n\u001b[1;32m     85\u001b[0m     exp_avgs,\n\u001b[1;32m     86\u001b[0m     exp_avg_sqs,\n\u001b[1;32m     87\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m     88\u001b[0m     state_steps\n\u001b[1;32m     89\u001b[0m ):\n\u001b[1;32m     90\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming `train_vocab` is defined and we have `data_train` and `data_val` datasets ready\n",
    "vocab_size = len(train_vocab)  # Ensure the vocab size is correct\n",
    "embedding_dim = 100  # Dimensionality of embedding space\n",
    "hidden_dim = 300  # Dimensionality of MLP hidden layer\n",
    "\n",
    "# Create the MLP model instance\n",
    "model3 = MLPAttentionModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "model3.to(device)\n",
    "data_loader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_model(model3, data_loader, val_loader, device, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daddc1f-a04d-485b-82d0-9e1e20c7e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in data_loader:\n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    print(classification_report(all_targets, all_preds, target_names=BE_HAVE_CONJUGATIONS))\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import itertools\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the best model\n",
    "cm = evaluate_model(model2, test_loader, device)\n",
    "plot_confusion_matrix(model2, BE_HAVE_CONJUGATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2822ea80-4821-4929-8a26-b1c0d4476abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They are waiting for the bus for over an hour', 'He are very happy now', 'I are 20 years old since may', 'They are one book', 'We are late', 'They are been waiting for the bus for over an hour']\n"
     ]
    }
   ],
   "source": [
    "# Assume model is trained and we have a mapping from targets to indices\n",
    "target_mapping = {\"be\": 0, \"am\": 1, \"are\": 2, \"is\": 3, \"was\": 4, \"were\": 5, \"been\": 6,\n",
    "                  \"being\": 7, \"have\": 8, \"has\": 9, \"had\": 10, \"having\": 11}\n",
    "\n",
    "# Function to map indices back to targets\n",
    "inv_target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "\n",
    "def preprocess_context(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    context_indices = [train_vocab[token] for token in tokens if token in train_vocab]\n",
    "    return context_indices\n",
    "\n",
    "def predict_blank(sentence):\n",
    "    model3.eval()\n",
    "    context_indices = preprocess_context(sentence)\n",
    "    if len(context_indices) < CONTEXT_SIZE:\n",
    "        return \"Insufficient context\"\n",
    "    context_tensor = torch.tensor([context_indices[:CONTEXT_SIZE]], dtype=torch.long).to(device)\n",
    "    output = model3(context_tensor)\n",
    "    _, predicted_index = torch.max(output, 1)\n",
    "    return inv_target_mapping[predicted_index.item()]\n",
    "\n",
    "\n",
    "sentences = [\n",
    "        \"They [blank] waiting for the bus for over an hour\",\n",
    "    \"He [blank] very happy now\",\n",
    "    \"I [blank] 20 years old since may\",\n",
    "    \"They [blank] one book\",\n",
    "    \"We [blank] late\",\n",
    "    \"They [blank] been waiting for the bus for over an hour\"]\n",
    "\n",
    "# Prediction for each sentence\n",
    "predicted_sentences = [s.replace(\"[blank]\", predict_blank(s)) for s in sentences]\n",
    "print(predicted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64781ab9-4be5-4f75-abcb-5a44744d8eaf",
   "metadata": {},
   "source": [
    "#### Part 2.3: Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aadee07c-6f39-487e-b129-41b3b3679503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        # Decode only the last output of the sequence for prediction of the next word\n",
    "        output = self.decoder(output[:, -1, :])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5985edd0-d863-4e64-9a1d-620785114fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_model(model, epochs, train_loader, vocab_size, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model.zero_grad()\n",
    "            output, _ = model(inputs)\n",
    "            loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print('Epoch: %d, Loss: %.4f' % (epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "479c5eeb-5a8d-4bab-94cb-0aed0cd023df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, vocab, device, initial_text, beam_width=3, n_words=10):\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    initial_tokens = tokenizer(initial_text)\n",
    "    stoi = vocab.get_stoi()\n",
    "    initial_indices = [stoi[token] if token in stoi else stoi[\"<unk>\"] for token in initial_tokens]\n",
    "    sequences = [[initial_indices, 0.0, None]]  # List of indexes, starting score, y hidden final state\n",
    "    \n",
    "    for _ in range(n_words):\n",
    "        all_candidates = list()\n",
    "        for seq in sequences:\n",
    "            seq_indices, score, hidden = seq[0], seq[1], seq[2]\n",
    "            input_tensor = torch.tensor([seq_indices[-1]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output, hidden = model(input_tensor, hidden)\n",
    "            softmax_scores = F.log_softmax(output, dim=-1)\n",
    "            top_indices = torch.topk(softmax_scores, beam_width, dim=-1)[1][0].tolist()\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                candidate_score = score - softmax_scores[0, idx].item()\n",
    "                candidate = [seq_indices + [idx], candidate_score, hidden]\n",
    "                all_candidates.append(candidate)\n",
    "        \n",
    "        # Select best candidates based on score\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "        sequences = ordered[:beam_width]\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    return [' '.join(itos[idx] for idx in seq[0]) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "18fe17a1-f61d-4a35-9106-578571e9921d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(data_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Entrenar modelo (se presupone que ya tienes un DataLoader configurado)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_rnn_model(rnn_model, \u001b[38;5;241m10\u001b[39m, train_loader, vocab_size, device)\n",
      "Cell \u001b[0;32mIn[130], line 15\u001b[0m, in \u001b[0;36mtrain_rnn_model\u001b[0;34m(model, epochs, train_loader, vocab_size, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "vocab_size = len(train_vocab)\n",
    "embed_size = 50  # Debe coincidir con el tamao del embedding utilizado en el entrenamiento\n",
    "hidden_size = 300\n",
    "num_layers = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "rnn_model = RNNModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "train_loader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "train_rnn_model(rnn_model, 10, train_loader, vocab_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "841d25e0-25af-4368-b574-8989a6bc0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man has discovered suddenly tells happy suddenly\n",
      "the man has discovered suddenly tells happy happy\n",
      "the man has discovered lines happy happy winter\n",
      "the man has discovered lines happy happy happy\n",
      "the man has discovered suddenly tells winter happy\n"
     ]
    }
   ],
   "source": [
    "# Play with the network to see how badly it generates text :).\n",
    "initial_text = \"The man has\"\n",
    "generated_texts = beam_search(rnn_model, train_vocab, device, initial_text, beam_width=5, n_words=5)\n",
    "for text in generated_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63e970-be3b-4370-a7ba-f07a3b72d917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "863ab29b-d70b-43a9-9c43-b6e7c43d22b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5989fa1-7235-420b-bd59-7e1379940934",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de09b1-9e46-427d-aa70-e8eeac855949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
